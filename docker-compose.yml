version: '3.7'
services:
    postgresql:
        image: docker.io/bitnami/postgresql:10
        volumes:
            - 'postgresql_data:/bitnami/postgresql'
        environment:
            - POSTGRESQL_DATABASE=bitnami_airflow
            - POSTGRESQL_USERNAME=bn_airflow
            - POSTGRESQL_PASSWORD=bitnami1
            # ALLOW_EMPTY_PASSWORD is recommended only for development.
            - ALLOW_EMPTY_PASSWORD=yes
    redis:
        image: docker.io/bitnami/redis:6.0
        volumes:
            - 'redis_data:/bitnami'
        environment:
            # ALLOW_EMPTY_PASSWORD is recommended only for development.
            - ALLOW_EMPTY_PASSWORD=yes
    airflow-scheduler:
        image: docker.io/bitnami/airflow-scheduler:2
        environment:
            - AIRFLOW_DATABASE_NAME=bitnami_airflow
            - AIRFLOW_DATABASE_USERNAME=bn_airflow
            - AIRFLOW_DATABASE_PASSWORD=bitnami1
            - AIRFLOW_EXECUTOR=CeleryExecutor
            - AIRFLOW_WEBSERVER_HOST=airflow
    airflow-worker:
        build: ./
        environment:
            - AIRFLOW_DATABASE_NAME=bitnami_airflow
            - AIRFLOW_DATABASE_USERNAME=bn_airflow
            - AIRFLOW_DATABASE_PASSWORD=bitnami1
            - AIRFLOW_EXECUTOR=CeleryExecutor
            - AIRFLOW_WEBSERVER_HOST=airflow
    airflow:
        build: ./
        environment:
            - AIRFLOW_DATABASE_NAME=bitnami_airflow
            - AIRFLOW_DATABASE_USERNAME=bn_airflow
            - AIRFLOW_DATABASE_PASSWORD=bitnami1
            - AIRFLOW_EXECUTOR=CeleryExecutor
            - LOAD_EX=n
            - EXECUTOR=Local
            - SPARK_MASTER_URL=spark://spark:7077
            - KAFKA_BROKER=kafka:9092
            - KAFKA_TOPIC=nimbus1
            - MYSQL_URL=db:2181
            - MYSQL_DB=nimbus
            - MYSQL_RAW_TABLE=stage1
            - MYSQL_UNIQUE_TABLE=stage2
            - MYSQL_DUPLICATE_TABLE=duplicates
            - MYSQL_USER=root
            - MYSQL_PASSWORD=asd
            - EMAIL=suchithald@gmail.com
            - INSTALL_PROVIDERS_FROM_SOURCES=true
            - _AIRFLOW_WWW_USER_USERNAME=nimbus
            - _AIRFLOW_WWW_USER_PASSWORD=nimbus
        logging:
            options:
                max-size: 10m
                max-file: "3"
        volumes:
            - ./src/main/dags:/usr/local/airflow/dags
            # - ./plugins:/usr/local/airflow/plugins
        ports:
            - "81:8080"
        command: webserver
        healthcheck:
            test:
                [
                    "CMD-SHELL",
                    "[ -f /usr/local/airflow/airflow-webserver.pid ]"
                ]
            interval: 30s
            timeout: 30s
            retries: 3

    spark:
        image: docker.io/bitnami/spark:3
        environment:
            - SPARK_MODE=master
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        ports:
            - '82:8080'
        volumes:
            - ./src:/examples/src/

    spark-worker:
        image: docker.io/bitnami/spark:3
        environment:
            - SPARK_MODE=worker
            - SPARK_MASTER_URL=spark://spark:7077
            - SPARK_WORKER_MEMORY=1G
            - SPARK_WORKER_CORES=1
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no

    db:
        image: mysql
        command: --default-authentication-plugin=mysql_native_password
        restart: always
        environment:
            MYSQL_ROOT_PASSWORD: asd
        ports:
            - "2181:2181"
        volumes:
        - ./mysql:/docker-entrypoint-initdb.d/

    phpMyAdmin:
        image: adminer
        restart: always
        ports:
            - 83:8080

    zookeeper:
        image: confluentinc/cp-zookeeper:7.0.0
        hostname: zookeeper
        environment:
            ZOOKEEPER_CLIENT_PORT: 2181
            ZOOKEEPER_TICK_TIME: 2000

    kafka:
        image: confluentinc/cp-kafka:7.0.0
        ports:
            - "9092:9092"
        depends_on:
            - zookeeper
        environment:
            KAFKA_BROKER_ID: 1
            KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
            KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
            KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://kafka:29092
            KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
            KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
            KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
        volumes:
        - /var/run/docker.sock:/var/run/docker.sock

    # Service to create kafka topic. 
    kafka-setup:
        image: confluentinc/cp-kafka:7.0.0
        hostname: kafka-setup
        depends_on:
        - kafka
        command: "bash -c 'echo Waiting for Kafka to be ready... && \
                        cub kafka-ready -b kafka:9092 1 20 && \
                        kafka-topics --create --if-not-exists --zookeeper zookeeper:2181 --partitions 1 --replication-factor 1 --topic purchases && \
                        /tmp/dashboard/docker-combined.sh'"
        environment:
            # The following settings are listed here only to satisfy the image's requirements.
            # We override the image's `command` anyways, hence this container will not start a broker.
            KAFKA_BROKER_ID: ignored
            KAFKA_ZOOKEEPER_CONNECT: ignored

volumes:
  postgresql_data:
    driver: local
  redis_data:
    driver: local